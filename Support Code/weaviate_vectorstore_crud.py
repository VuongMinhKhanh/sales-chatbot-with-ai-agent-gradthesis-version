# -*- coding: utf-8 -*-
"""Weaviate Vectorstore CRUD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ivhcofWkC-BO8WyVuCIaNcEPcLoxRst1
"""

!pip install -q langchain langchain_community

from google.colab import drive
drive.mount('/content/drive')

# import pandas as pd

# csv_path = "/content/drive/My Drive/Finished_Data_in_769audio_vn.csv"
# df = pd.read_csv(csv_path, encoding="utf-8")

!pip install -q --upgrade gspread oauth2client

import gspread
from google.colab import auth
from oauth2client.client import GoogleCredentials
from google.auth import default
import csv

# Authenticate and create a client
auth.authenticate_user()
creds, _ = default()
gc = gspread.authorize(creds)

# Open the Google Sheets file
spreadsheet = gc.open("769audio dataset")

worksheet = spreadsheet.worksheet("Filtered Dataset")
data = worksheet.get_all_values()

import pandas as pd

df = pd.DataFrame(data[1:], columns=data[0])

len(df)

# import pandas as pd

# xlsx_path = "/content/drive/MyDrive/769audio dataset.gsheet"
# df = pd.read_excel(xlsx_path)

# df.head()

import re
import numpy as np

# Function to clean and format text
def format_text(text):
    # Normalize new lines, replacing multiple new lines with a single new line
    if text is np.nan:
        return None

    # print(text)
    text = re.sub(r'\n\s*\n', '\n', text.strip())

    # Normalize whitespace within lines
    text = re.sub(r'[ \t]+', ' ', text)
    # Capitalize the first letter of each sentence in each paragraph separately
    paragraphs = text.split('\n')
    formatted_paragraphs = []
    for paragraph in paragraphs:
        sentences = re.split(r'(?<=[.!?]) +', paragraph.strip())
        formatted_sentences = [sentence.capitalize() for sentence in sentences]
        formatted_paragraph = '. '.join(formatted_sentences)
        formatted_paragraphs.append(formatted_paragraph)
    # Rejoin paragraphs with a single newline
    text = '\n'.join(formatted_paragraphs)
    # Ensure proper spacing after punctuations
    text = re.sub(r'\s([,.])', r'\1', text)
    text = re.sub(r'([,.])([^\s])', r'\1 \2', text)
    # Replace non-breaking space with a regular space
    text = text.replace('\xa0', ' ')
    return text

new_df = df.copy()

# Apply formatting function
df['Mô tả'] = df['Mô tả'].apply(format_text)
df['Mô tả chi tiết'] = df['Mô tả chi tiết'].apply(format_text)
df['Nội dung'] = df['Nội dung'].apply(format_text)

# from langchain.text_splitter import RecursiveCharacterTextSplitter
# from langchain.docstore.document import Document

# # Initialize a list to store the documents
# documents_json = []

# # Initialize the text splitter with overlap
# text_splitter = RecursiveCharacterTextSplitter(
#     separators=["\n", " ", ".", ","],
#     chunk_size=65,
#     chunk_overlap=2**4
# )

# # Iterate through each row in the DataFrame
# for row_number, row_data in df.iterrows():
#     # Columns to process normally
#     columns_to_process = ["Tên", "link sản phẩm", "Giá", "Link ảnh", "Tình trạng"]

#     for col in columns_to_process:
#         # Create a Document for each cell
#         documents_json.append(Document(
#             page_content=f"{col}: {str(row_data[col])}",
#             metadata={"source": col, "row": row_number}
#         ))

#     # Process Tập link ảnh by splitting links
#     if pd.notna(row_data["Tập link ảnh"]):
#         links = str(row_data["Tập link ảnh"]).split(',')
#         for link in links:
#             documents_json.append(Document(
#                 page_content=link,
#                 metadata={"source": "Tập link ảnh", "row": row_number}
#             ))

#     # Process Giới thiệu and Chi tiết by splitting texts using RecursiveCharacterTextSplitter
#     for col in ["Giới thiệu", "Chi tiết"]:
#         if pd.notna(row_data[col]):
#             # Create a list of documents for the text
#             initial_document = Document(
#                 page_content=row_data[col],
#                 metadata={"source": col, "row": row_number}
#             )
#             text_chunks = text_splitter.split_documents([initial_document])

#             # Add order count to metadata
#             for index, chunk in enumerate(text_chunks):
#                 documents_json.append(Document(
#                     page_content=chunk.page_content,
#                     metadata={
#                         "source": col,
#                         "row": row_number,
#                         "chunk_index": index  # Add the order count here
#                     }
#                 ))

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
import pandas as pd

# Initialize a list to store the documents
documents_json = []

# Initialize the text splitter with overlap
text_splitter = RecursiveCharacterTextSplitter(
    separators=["\n", " ", ".", ","],
    chunk_size=65,
    chunk_overlap=2**4
)

# Columns to process normally
# Updated columns to process normally based on new dataset headers
columns_to_process = [
    "ID",
    "Mặt hàng",
    "Danh mục",
    "Quảng cáo",
    "Mới",
    "Nổi bật",
    "Hàng sắp về",
    "Hết hàng",
    "Thương hiệu",
    "Nhãn hiệu",
    "Tính năng",
    "Tên",
    "Giá gốc",
    "Bảo hành",
    "Hiệu suất",
    "Tiêu đề",
    "Từ khóa",
    "Thẻ",
    "Hiển thị",
    "Lượt xem",
    "ID cùng loại",
    "Chất liệu",
    "Ghi chú",
    "Mua kèm 1",
    "Đơn vị",
    "Sản phẩm top 10",
    "Link sản phẩm",
]

# Update image columns (new header for image links)
image_columns = ["Danh sách link ảnh"]

# Columns to split into chunks for longer text fields
text_columns = [
    "Mô tả",
    "Mô tả chi tiết",
    "Nội dung",
    "Nội dung nổi bật",
    "Nội dung nổi bật 1",
    "Nội dung nổi bật 2",
    "Nội dung nổi bật 3",
    "Nội dung nổi bật 4",
    "Nội dung nổi bật 5",
    "Nội dung nổi bật 6",
    "Nội dung nổi bật 7",
    "Nội dung nổi bật 8",
    "Nội dung nổi bật 9",
]

# Iterate through each row in the DataFrame
for row_number, row_data in df.iterrows():
    # Process normally segmented columns
    for col in columns_to_process:
        if pd.notna(row_data[col]):
            documents_json.append(Document(
                page_content=f"{col}: {str(row_data[col])}",
                metadata={"source": col, "row": row_number}
            ))

    # Process image columns
    for img_col in image_columns:
        if pd.notna(row_data[img_col]):
            links = str(row_data[img_col]).split(',')
            for link in links:
                documents_json.append(Document(
                    page_content=link.strip(),
                    metadata={"source": img_col, "row": row_number}
                ))

    # Process long text columns with chunk splitting
    for col in text_columns:
        if pd.notna(row_data[col]):
            initial_document = Document(
                page_content=row_data[col],
                metadata={"source": col, "row": row_number}
            )
            text_chunks = text_splitter.split_documents([initial_document])
            # Add each chunk as a separate document with its index
            for index, chunk in enumerate(text_chunks):
                documents_json.append(Document(
                    page_content=chunk.page_content,
                    metadata={"source": col, "row": row_number, "chunk_index": index}
                ))

documents = documents_json[0:50]
documents

# for doc in documents:
#   print(doc)
# documents_json[0:15]

# !pip uninstall -q weaviate-client langchain-weaviate

!pip install -q weaviate-client

!pip install -q langchain-weaviate

import os
os.environ["OPENAI_API_KEY"] = ""

!pip install -q langchain_openai

from langchain_openai import OpenAIEmbeddings
embeddings = OpenAIEmbeddings(model="text-embedding-3-large") # OpenAIEmbeddings()

# !pip uninstall -q weaviate

# !pip install -q weaviate

import weaviate
# from langchain_weaviate.vectorstores import WeaviateVectorStore
# import weaviate.classes as wvc
from weaviate.classes.init import Auth

os.environ["WEAVIATE_URL"] = ""
os.environ["WEAVIATE_API_KEY"] = ""

weaviate_url = os.environ["WEAVIATE_URL"]
weaviate_api_key = os.environ["WEAVIATE_API_KEY"]
openai_api_key = os.environ["OPENAI_API_KEY"]

# Connect to Weaviate Cloud
client = weaviate.connect_to_weaviate_cloud(
    cluster_url=weaviate_url,
    auth_credentials=Auth.api_key(weaviate_api_key),
    headers={"X-OpenAI-Api-Key": openai_api_key}
)

# from langchain_weaviate.vectorstores import WeaviateVectorStore

# docsearch = WeaviateVectorStore(
#     embedding=embeddings,
#     client=weaviate_client,
#     index_name="TestCollection",
#     text_key="text"
# )

from weaviate.classes.config import Configure, Property, DataType, Tokenization


collection = client.collections.create(
    "ChatBot_769Audio",
    properties=[
        Property(
            name="text",
            data_type=DataType.TEXT,
            vectorize_property_name=True,  # Use "title" as part of the value to vectorize
        ),
        Property(
            name="row",
            data_type=DataType.INT,
            skip_vectorization=True,  # Don't vectorize this property
        ),
        Property(
            name="column",
            data_type=DataType.TEXT,
            skip_vectorization=True,  # Don't vectorize this property
        ),
        Property(
            name="chunk_index",
            data_type=DataType.INT,
            skip_vectorization=True,  # Don't vectorize this property
        ),
    ]
)

documents = documents_json[:]

with collection.batch.dynamic() as batch:
    for doc in documents:
        weaviate_obj = {
          "text": doc.page_content,
          "row": doc.metadata["row"],
          "column": doc.metadata["source"],
        }

        # Include 'chunk_index' if applicable
        if doc.metadata["source"] in text_columns:
            weaviate_obj["chunk_index"] = doc.metadata.get("chunk_index")

        # print(weaviate_obj)
        # The model provider integration will automatically vectorize the object
        batch.add_object(
            properties=weaviate_obj,
        )

# print(len(documents_json))

"""Get products by pagination"""

# Define the function to fetch a specific page
# data-missed vectorstore
# LangChain_42df72b00265499a84d4c193b6b5f3b4
# LangChain_7f6cbcaeb9f94b1aae10fac59e421e3c
# LangChain_d503be4e4ee04591b1371e0268672b26
# LangChain_c4a12412bb56403fb610e15648e49aee
# test vectorstore
# LangChain_75fbe992081c41fbbbdc1a3615046dda
collection_name = "ChatBot_769Audio"

def fetch_page(weaviate_client, page_number=0, page_size=10):
    # Calculate the offset based on the page number and page size
    offset = page_number * page_size
    limit = page_size

    # Construct the GraphQL query with pagination
    query = f"""
    {{
      Get {{
        {collection_name}(
          where: {{
            operator: And
            operands: [
              {{
                path: ["row"]
                operator: GreaterThanEqual
                valueInt: 90 # {offset}
              }},
              {{
                path: ["row"]
                operator: LessThanEqual
                valueInt: 100 # {offset + limit}
              }}
            ]
          }}
          limit: 10000
          # offset: {offset}
        ) {{
          text
          row
          column
          chunk_index
          _additional {{
            id,
          }}
        }}
      }}
    }}
    """

    # Execute the query
    response = weaviate_client.graphql_raw_query(query)
    # print(response)
    results = response.get[collection_name]

    # Return the results
    return results

# Assuming you have a Weaviate client instance named 'weaviate_client'
page_number = 0  # Start from page 0
page_size = 10   # Default page size

# Fetch the first page
results = fetch_page(client, page_number, page_size)

# Print the results
for res in results:
  if res['column'] == 'Tên':
    print(f"Text: {res['text']}, column: {res['column']}, UUID: {res['_additional']['id']}") # , Row: {res['row']}, index: {res['chunk_index']}, UUID: {res['_additional']['id']}

# # Function to reconstruct paragraphs
# def reconstruct_paragraph(tokens, source_name, row_number):
#     # Filter tokens for the given source and row
#     filtered_tokens = [
#         token for token in tokens
#         if token['source'] == source_name and token['row'] == row_number
#     ]
#     # Ensure that 'chunk_index' is not None
#     filtered_tokens = [token for token in filtered_tokens if token['chunk_index'] is not None]
#     # Sort tokens by 'chunk_index'
#     sorted_tokens = sorted(filtered_tokens, key=lambda x: x['chunk_index'])
#     # Concatenate 'text' fields
#     paragraph = ' '.join(token['text'] for token in sorted_tokens)
#     return paragraph

# # Reconstruct 'Giới thiệu' paragraph
# gioi_thieu_paragraph = reconstruct_paragraph(results, 'Giới thiệu', 0)

# # Reconstruct 'Chi tiết' paragraph
# chi_tiet_paragraph = reconstruct_paragraph(results, 'Chi tiết', 0)

# # Output the reconstructed paragraphs
# print("Giới thiệu Paragraph:")
# print(gioi_thieu_paragraph)
# print("\nChi tiết Paragraph:")
# print(chi_tiet_paragraph)

"""Find products by keywords"""

def find_products_by_keyword(weaviate_client, keyword):
  # Construct the GraphQL query
    query = f"""
    {{
      Get {{
        {collection_name}(
          where: {{
            path: ["text"]
            operator: Like
            valueString: "%{keyword}%"
          }}
        ) {{
          row
        }}
      }}
    }}
    """

    # Execute the query
    response = weaviate_client.graphql_raw_query(query)
    # print(response)
    results = response.get[collection_name]
    print(results)
    # Return the results
    return results

keyword = "cd"

# Find entries by text key
results = find_products_by_keyword(weaviate_client, keyword)
rows = list(set([res['row'] for res in results]))
# results
# Print the results
for row in rows:
    print(f"Row: {row}")

def find_products_by_rows(weaviate_client, row_indexes):
    or_conditions = ', '.join([f'{{ path: ["row"], operator: Equal, valueInt: {row} }}' for row in row_indexes])

    # Construct the GraphQL query
    query = f"""
    {{
      Get {{
        {collection_name}(
          where: {{
            operator: Or
            operands: [
              {or_conditions}
            ]
          }}
          limit: 10000
        ) {{
          text
          row
          column
          chunk_index
          _additional {{
            id
          }}
        }}
      }}
    }}
    """

    # Execute the query
    response = weaviate_client.graphql_raw_query(query)
    # print(response)
    results = response.get[collection_name]

    # Return the results
    return results

print(rows)

results = find_products_by_rows(weaviate_client, [0, 1, 2, 3, 4, 5, 6, 8])
for res in results:
  if res['column'] == 'Tên':
    print(f"Text: {res['text']}, Row: {res['row']}, index: {res['chunk_index']}, column: {res['column']}, UUID: {res['_additional']['id']}")

"""Update product attribute"""

# Function to join texts by chunk_index, including overlaps
def join_texts_with_overlaps(token_list):
    # Sort tokens by 'chunk_index' to maintain the correct order
    sorted_tokens = sorted(token_list, key=lambda x: x['chunk_index'])

    # Concatenate all texts into a single string
    result = ' '.join(token['text'] for token in sorted_tokens)

    return result

# Filter tokens for 'Chi tiết' column and row 0
tokens = [res for res in results if res["column"] == "Chi tiết" and res["row"] == 0]

# Join the texts into a single paragraph including overlaps
original_paragraph = join_texts_with_overlaps(tokens)
print("Original Paragraph (with overlaps):")
print(original_paragraph)

# Function to join texts while avoiding overlapping words
def join_texts_by_chunk_index(token_list):
    # Sort tokens by chunk_index to maintain the correct order
    sorted_tokens = sorted(token_list, key=lambda x: x['chunk_index'])

    result = []
    previous_text = ""

    for token in sorted_tokens:
        current_text = token['text']

        # Check if there's overlap between the end of the previous text and the start of the current text
        overlap_length = 0
        for i in range(min(len(previous_text.split()), len(current_text.split()))):
            if previous_text.split()[-(i+1):] == current_text.split()[:i+1]:
                overlap_length = i + 1

        if overlap_length > 0:
            result.append(' '.join(current_text.split()[overlap_length:]))
        else:
            result.append(current_text)

        previous_text = current_text

    return ' '.join(result)

# Join the texts into a single paragraph with no overlapping words

tokens = [res for res in results if res["column"] == "Chi tiết" and res["row"] == 0]
original_paragraph = join_texts_by_chunk_index(tokens)
print("Processed Paragraph (no overlaps):")
print(original_paragraph)

import difflib

def detect_and_update_changes(data, updated_paragraph):
    """Detect which text has been updated and return updated tokens."""
    updated_tokens = []

    # Concatenate the original tokens to form the original text
    original_text = ' '.join([token['text'].strip() for token in data])
    original_words = original_text.split()
    updated_words = updated_paragraph.split()

    # Use SequenceMatcher to find matching blocks
    matcher = difflib.SequenceMatcher(None, original_words, updated_words)
    blocks = matcher.get_matching_blocks()

    # Map each token to its word indices in the original text
    token_word_indices = []
    idx = 0
    for token in data:
        token_words = token['text'].strip().split()
        num_words = len(token_words)
        token_word_indices.append((idx, idx + num_words))
        idx += num_words

    # For each token, check if the words have changed
    for token_idx, (start, end) in enumerate(token_word_indices):
        original_token_words = original_words[start:end]
        updated_token_words = updated_words[start:end]
        token_text = ' '.join(original_token_words)
        updated_token_text = ' '.join(updated_token_words)

        if original_token_words != updated_token_words:
            # Token has changed
            print(f"Token '{token_text}' has changed to '{updated_token_text}'")
            updated_tokens.append({
                "uuid": data[token_idx]['_additional']['id'],
                "updated_text": updated_token_text,
            })
        else:
            print(f"Token '{token_text}' is unchanged")

    print("Updated tokens:", updated_tokens)
    return updated_tokens

# def match_token_in_substring(token, substring, return_start=False):
#     """Match token in substring allowing for whitespace differences.

#     If return_start is True, returns the start index of the match relative to substring.
#     Otherwise, returns the end index of the match.
#     """
#     token_escaped = re.escape(token)
#     token_pattern = re.sub(r'\\\s+', r'\\s+', token_escaped)
#     pattern = re.compile(token_pattern)
#     match = pattern.match(substring)
#     if match:
#         return match.start() if return_start else match.end()
#     else:
#         return None

# def extract_updated_text(original_text, substring):
#     """Extract the updated text corresponding to the original token."""
#     # Split original text and substring into words
#     original_words = original_text.split()
#     substring_words = substring.split()

#     # Compare words to find where they diverge
#     updated_words = []
#     for o_word, s_word in zip(original_words, substring_words):
#         if o_word != s_word:
#             updated_words.append(s_word)
#         else:
#             updated_words.append(s_word)

    # # Handle case where the substring has extra words (e.g., changed token is longer)
    # if len(substring_words) > len(original_words):
    #     additional_words = substring_words[len(original_words):]
    #     updated_words.extend(additional_words)

    # updated_text = ' '.join(updated_words[:len(original_words)])
    # return updated_text

# Sample token data
def remove_overlapping_words(tokens):
    """Remove overlapping words from the next token's text in the token list."""
    for i in range(len(tokens) - 1):
        current_text = tokens[i]['text']
        next_text = tokens[i + 1]['text']

        # Split texts into words
        current_words = current_text.split()
        next_words = next_text.split()

        # Find maximum overlap between the end of current_words and the start of next_words
        max_overlap = 0
        max_k = min(len(current_words), len(next_words))
        for k in range(1, max_k + 1):
            if current_words[-k:] == next_words[:k]:
                max_overlap = k

        if max_overlap > 0:
            # Remove overlapping words from the start of next_text
            overlapping_words = next_words[:max_overlap]
            new_next_text = ' '.join(next_words[max_overlap:])
            # Update next token's text
            tokens[i + 1]['text'] = new_next_text
            print(f"Removed overlapping words from token {i + 1}: {' '.join(overlapping_words)}")

    return tokens

# Remove overlapping words
print("original", tokens)
tokens = remove_overlapping_words(tokens)
tokens = sorted(tokens, key=lambda x: x['chunk_index'])
for token in tokens:
  print(token["text"])
# Updated paragraph (with "hình" inserted before "ảnh")
updated_paragraph = """Chi tiết sản phẩm
Mô tả
Đầu cd denon dcd qs10ii đẹp xuất sắc Xuất xứ: japan, bảo hành 2 tháng Cd denon dcd s10ii xuất sắc, điện 200v, có remote - hình thức đẹp, hàng tuyển chọn kỹ lưỡng. Cd denon dcd s10ii là dòng sản phẩm cao cấp của hãng denon, bộ cơ bọc đồng chống nhiễu, hai tăng phô nguồn, toàn bộ tụ elna for audio chất lượng. . Chất âm dày dặn, chi tiết, chơi tốt nhiều dòng nhạc Liên hệ 769audio để được tư vấn và giao hàng miễn phí Đt : 0916142465 -  (08) 62 948 827 Chúng tôi có nhận giao bán hàng ở tỉnh 769audio cám ơn qúy khách đã ghé thăm sản phẩm !
"""

# Detect improper URL form, no change if URL is not correct
# Pass any space

# Detect and print updated tokens
updated_tokens = detect_and_update_changes(tokens, updated_paragraph)
# Output the tokens that need updating
print()
print(updated_tokens)
for token in updated_tokens:
    print(f"UUID: {token['uuid']}, Updated Text: {token['updated_text']}")

for token in updated_tokens:
  collection.data.update(
      uuid=token['uuid'],
      properties={
          "text": token['updated_text'],
      }
  )

"""Delete a product by replacing all data of that row to blank"""

def get_all_uuid_by_row(weaviate_client, row_index):
    # or_conditions = ', '.join([f'{{ path: ["row"], operator: Equal, valueNumber: {row} }}' for row in row_indexes])

    # Construct the GraphQL query
    query = f"""
    {{
      Get {{
        {collection_name}(
          where: {{
            path: ["row"]
            operator: Equal
            valueInt: {row}
          }}
        ) {{
          _additional {{
            id
          }}
        }}
      }}
    }}
    """

    # Execute the query
    response = weaviate_client.graphql_raw_query(query)

    results = response.get[collection_name]

    # Return the results
    return results

uuids = get_all_uuid_by_row(weaviate_client, 0)
ids = [uuid['_additional']['id'] for uuid in uuids]
ids

for uuid in uuids:
  collection.data.update(
      uuid=uuid["_additional"]["id"],
      properties={
          "text": "",
      }
  )

from weaviate.classes.query import Filter

collection.data.delete_many(
    where=Filter.by_id().contains_any(ids)
)