# -*- coding: utf-8 -*-
"""Qdrant vectorstore.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_wdwSb0B4JOZIeSsA9486eVOR9dpBWKk
"""

!pip install qdrant-client

!pip install gspread oauth2client langchain_openai

!pip install fastembed

# --- Configuration ---
QDRANT_CLOUD_URL = ""
QDRANT_API_KEY = ""
SPREADSHEET_FILE = "User Feedback"  # your structured spreadsheet
COLLECTION_NAME = "user_feedback"
VECTOR_SIZE = 3072
BATCH_SIZE = 32

import os
os.environ["OPENAI_API_KEY"] = ""

from qdrant_client import QdrantClient, models

# --- 1. Connect to Qdrant ---
# If you're using Qdrant Cloud:
qdrant_client = QdrantClient(url=QDRANT_CLOUD_URL, api_key=QDRANT_API_KEY)

# --- 2. Create Collection ---
from qdrant_client.http.models import CollectionStatus, PointStruct

try:
    collection_info = qdrant_client.get_collection(collection_name=COLLECTION_NAME)
    if collection_info.status != CollectionStatus.GREEN:
        print(f"Collection {COLLECTION_NAME} exists but is not ready. Waiting...")
    else:
        print(f"Collection {COLLECTION_NAME} already exists")
except Exception:
    qdrant_client.create_collection(
        collection_name=COLLECTION_NAME,
        vectors_config=models.VectorParams(size=VECTOR_SIZE, distance=models.Distance.COSINE)
    )
    print(f"Collection {COLLECTION_NAME} is created")

# # --- 3. Load Spreadsheet ---
# from google.colab import drive
# drive.mount("/content/drive")

# import pandas as pd

# df = pd.read_excel(f"/content/drive/MyDrive/{SPREADSHEET_FILE}")
# # df.head()

# --- 3. Load Spreadsheet ---
import gspread
from google.colab import auth
from oauth2client.client import GoogleCredentials
from google.auth import default
import csv

# Authenticate and create a client
auth.authenticate_user()
creds, _ = default()
gc = gspread.authorize(creds)

# Open the Google Sheets file
spreadsheet = gc.open("User Feedback")

worksheet = spreadsheet.worksheet("Enriched Feedback")
data = worksheet.get_all_values()

import pandas as pd

feedback_df = pd.DataFrame(data[1:], columns=data[0])

# --- 4. Load Embeddings ---
from langchain_openai import OpenAIEmbeddings

embedding_model = OpenAIEmbeddings(model="text-embedding-3-large")

# --- 5. Iterate, Embed, and Upsert for User Feedback ---
points = []
for i in range(0, len(feedback_df), BATCH_SIZE):
    batch_df = feedback_df.iloc[i: i + BATCH_SIZE]

    # Construct text to embed by combining key fields from the user feedback record.
    texts = [
        f"Query: {row['Query']} Feedback: {row['Feedback']} Correction: {row['Correction']}"
        for _, row in batch_df.iterrows()
    ]

    # Compute embeddings for the batch of texts.
    embeddings = [embedding_model.embed_query(text) for text in texts]

    for j, embedding in enumerate(embeddings):
        row = batch_df.iloc[j]
        # Build a payload with relevant metadata
        payload = {
            # Use a unique id; if 'id' column doesn't exist, use the batch index as fallback.
            "id": row.get("id", i + j),
            "workflow_stage": row["workflow_stage"],
            "Query": row["Query"],
            "Response": row["Response"],
            "Feedback": row["Feedback"],
            "Correction": row["Correction"],
            "timestamp": row["timestamp"]
        }

        points.append(
            PointStruct(
                id=i + j,
                vector=embedding,
                payload=payload
            )
        )

    print(f"Processed batch {i // BATCH_SIZE + 1}")

# === 6. Upsert to Qdrant ===
qdrant_client.upsert(collection_name=COLLECTION_NAME, wait=True, points=points)
print(f"✅ Uploaded {len(points)} user feedback to Qdrant.")

# === 7. Example Query (Optional) ===
query = "cho tôi xem hàng với"
query_embedding = embedding_model.embed_query(query)

from qdrant_client.http.models import Filter, FieldCondition, MatchValue

# Optional filter by workflow_stage
query_filter = Filter(
    must=[
        FieldCondition(
            key="workflow_stage",
            match=MatchValue(value="Qualification")
        )
    ]
)

# Final query
results = qdrant_client.query_points(
    collection_name=COLLECTION_NAME,
    query=query_embedding,
    limit=5,
    query_filter=query_filter  # Optional
)

import time

start = time.time()

print("Results:")
for result in results.points:
    print(f"- {result.payload} (Score: {result.score:.4f})")

end = time.time()
print(f"Time taken: {end - start:.4f} seconds")

"""#Test with AI Agents"""

!pip install -q langchain langchain_community gspread oauth2client langchain_experimental chromadb langchain_openai faiss-cpu

!pip install -q weaviate-client langchain-weaviate

premise = """
You are a highly qualified sales consultant specializing in electronic music equipment. You are working for 769Audio, one of the top three distributors in Ho Chi Minh City. Your goal is to provide accurate, helpful, and professional responses to customers based on the provided database.

---

### **Role:**
- Your primary role is to assist customers in selecting and understanding products from the database.

---

### **Context:**
- You are communicating with customers in Vietnamese unless they explicitly request another language.
- Always base your answers on the provided database. Do not generate any additional or speculative information.

---

### **Key Instructions:**

#### **General Product Queries:**
- Use the product name in the `"Tên"` column to identify matches, even if phrased differently by the customer.
- Whenever **a product is mentioned**, always provide **two types of links** for products:
  1. The main product link ("Link sản phẩm").
  2. Additional image links from `"Tập link ảnh"`. Include up to 3 relevant links if available.
- Format image links neatly in Markdown. Example:
Bạn có thể xem chi tiết và hình ảnh sản phẩm tại đây:
**Link sản phẩm:**
[Loa JBL Pasion 10](https://saigonaudio.com.vn/san-pham/123/loa-jbl-pasion-10.html)

**Hình ảnh sản phẩm:**
[![Hình 1](https://saigonaudio.com.vn/upload/images/Loa-JBL-Pasion-10-01.jpg)](https://saigonaudio.com.vn/upload/images/Loa-JBL-Pasion-10-01.jpg)
[![Hình 2](https://saigonaudio.com.vn/upload/images/Loa-JBL-Pasion-10-02.jpg)](https://saigonaudio.com.vn/upload/images/Loa-JBL-Pasion-10-02.jpg)

#### **Price Details:**
- When quoting prices, prioritize promotions:
1. Check the `"Khuyến mãi"` column. If the value is `1`, the product is on discount.
2. Look in the `"Nội dung"` column for any additional promotion details.
3. Quote both the original price from the `"Giá gốc"` column and the discounted price if applicable.

#### **Availability:**
- Only recommend products where the `"Hiển thị"` column indicates `1` (visible) or the `"Tình trạng"` column indicates `1` (in stock).
- Politely inform the customer if a product is unavailable.

#### **Features and Specifications:**
- For questions about specifications like capacity or other attributes, first check the `"Mô tả"` and `"Nội dung"` columns.
- If no relevant details are found in these columns, clearly state that the information is not available in the database.

#### **Country of Origin:**
- If a product is manufactured in China, first describe it as imported. If the customer asks for more detail, directly mention "manufactured in China."

---

### **Markdown and Formatting:**
- Format your responses neatly using line breaks and Markdown where appropriate.
- When providing image links, render the image in Markdown so it is visible in the chat, and make the image clickable by embedding it with a link.
- Example:
Question: "What is the price of Loa JBL 201 Series 4?"
Response: Dạ, hiện tại, giá của loa là 3,500,000₫. Hiện đang giảm còn 3,000,000₫:
**Link sản phẩm:**
[Loa JBL 201 Series 4](https://saigonaudio.com.vn/san-pham/201/loa-jbl-201-series-4.html)

**Hình ảnh sản phẩm:**
[![Hình 1](https://saigonaudio.com.vn/upload/images/Loa-JBL-201-Series-4-01.jpg)](https://saigonaudio.com.vn/upload/images/Loa-JBL-201-Series-4-01.jpg)
[![Hình 2](https://saigonaudio.com.vn/upload/images/Loa-JBL-201-Series-4-02.jpg)](https://saigonaudio.com.vn/upload/images/Loa-JBL-201-Series-4-02.jpg)

---

### **Customer-Centric Assistance:**
- Always prioritize promotions and discounts when suggesting products.
- If a customer indicates they want to purchase something, suggest relevant items first based on their query.
- If the customer specifies a price range, look for products with prices closest to their range.
- Use examples from the database for clarity. Always provide relevant URLs for products.

---

### **Advanced Document Understanding:**
- When searching for discount or promotion details:
  - Look at the metadata `source: "Khuyến mãi"`. If the value is `1`, the product has a discount.
  - Cross-reference the metadata `source: "Nội dung"` for additional details about promotions or discounts.

- If the question involves finding related specifications:
  - First prioritize `"Mô tả"` and `"Nội dung"`.
  - If these do not contain the required details, inform the customer.

- If a product image is requested:
  - Provide the link from `"Tập link ảnh"` or `"Ảnh chính"` or `"Ảnh nhỏ"`.
  - Example: "Here's the image: [![Image](https://saigonaudio.com.vn/upload/images/<Ảnh chính>.jpg)](https://saigonaudio.com.vn/upload/images/<Ảnh chính>.jpg)."

---

### **Notes:**
- Always prioritize database information. Never guess or fabricate data.
- Clearly admit when no relevant information is available: "Xin lỗi, nhưng hiện tại tôi không tìm thấy thông tin phù hợp..."
- Respond succinctly, professionally, and engagingly.
"""

import json
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

def contextualize_query_and_detect_phase(query, chat_history, user_profile):
    """
    Rephrases a user's query for clarity and determines the sales workflow stage.

    The prompt instructs the LLM to:
      - Rephrase the customer's query using the conversation history.
      - Inject discount keywords when customers ask for product suggestions.
      - Analyze the chat history and user profile to decide the current sales stage.
      - Always respond in Vietnamese (unless the customer specifies otherwise).

    Few-shot examples (in Vietnamese) are provided to illustrate the expected behavior.
    Expected JSON response format:
      {"rephrased_query": "…", "workflow_stage": "…"}
    """

    # Combined unified prompt with instructions, examples, and a JSON sample.
    unified_system_prompt = f"""
Bạn là một chuyên gia tư vấn bán hàng thiết bị âm thanh (loa, micro, mixer, ampli,...) với kiến thức sâu về quy trình bán hàng. Quy trình của chúng ta gồm các giai đoạn: {get_workflow_stages()}.
    """ + """
Hãy thực hiện các nhiệm vụ sau:
Dựa trên lịch sử trò chuyện dưới đây và câu hỏi mới nhất của khách hàng, hãy diễn giải câu hỏi sao cho dễ hiểu và liên quan đến ngữ cảnh đã trao đổi.

1. Sử dụng thông tin từ lịch sử để thêm chi tiết còn thiếu cho câu hỏi mới (nếu có).
2. Đảm bảo rằng câu hỏi được diễn giải một cách chính xác và ngắn gọn, nhưng vẫn giữ ngữ cảnh từ lịch sử trò chuyện.
3. Nếu không có đủ thông tin từ lịch sử, hãy diễn giải câu hỏi mới sao cho dễ hiểu nhất mà không cần ngữ cảnh.
4. Phân tích lịch sử trò chuyện và hồ sơ khách hàng để xác định giai đoạn hiện tại của quy trình bán hàng.
5. Nếu lịch sử cho thấy đã có 2–3 vòng trao đổi (ví dụ: nhiều tương tác ở giai đoạn Needs Assessment hoặc Qualification), hãy dự đoán cuộc trò chuyện chuyển sang giai đoạn Product Presentation.
6. Nếu không đủ thông tin từ lịch sử, hãy tái diễn đạt lại câu hỏi mới sao cho đơn giản và dễ hiểu.

Ví dụ:
- Ví dụ 1:
  + Lịch sử: "Cho tôi biết giá loa JBL 201 Series 4"
  + Câu hỏi mới: "Gửi link sản phẩm"
  + Diễn giải (kết quả mong đợi): {{"rephrased_query": "Gửi link sản phẩm", "workflow_stage": "Needs Assessment"}}

- Ví dụ 2:
  + Lịch sử: "Tôi muốn mua micro cao cấp"
  + Câu hỏi mới: "Bao nhiêu tiền micro?"
  + Diễn giải (kết quả mong đợi): {{"rephrased_query": "Bao nhiêu tiền micro?", "workflow_stage": "Needs Assessment"}}

Lịch sử trò chuyện:
{chat_history}

Mô tả khách hàng:
{user_profile}

Câu hỏi mới:
{input}

Trả lời của bạn phải là một đối tượng JSON với định dạng:
{{"rephrased_query": "…", "workflow_stage": "…"}}
    """

    # Build the unified prompt with a single system message and placeholders.
    unified_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", unified_system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),  # Placeholder for dynamic chat history
            ("system", "{user_profile}"),  # Inject the user profile into the prompt
            ("human", "{input}"),
        ]
    )

    def contextualize_query(query, history, user_profile):
        prompt = unified_prompt.format(input=query, chat_history=history, user_profile=user_profile)
        return llm.invoke(prompt)  # 'llm' is your language model interface

    # Invoke the language model and parse the JSON response.
    contextualized_query = contextualize_query(query, chat_history, user_profile)
    json_data = json.loads(contextualized_query.content)
    return json_data


def get_workflow_stages():
  workflow_data = {
    "Greeting": {
        "internalInstruction": "Greet the user warmly, introduce the service or product line, and invite them to share their needs or questions.",
        "sampleUserMessage": "Hello there! How can I assist you with your audio equipment needs today?"
    },
    "Needs Assessment": {
        "internalInstruction": "Ask targeted questions to uncover the user's main challenges, goals, or requirements. Focus on clarifying their exact needs.",
        "sampleUserMessage": "Could you tell me more about what you're looking for and any challenges you’re currently facing?"
    },
    "Qualification": {
        "internalInstruction": "Determine if the customer meets key criteria, such as budget, timeline, or decision-making authority. Gather enough info to decide if they’re a good fit.",
        "sampleUserMessage": "Do you have a budget range or timeframe in mind? And are you the one making the final decision?"
    },
    "Presentation": {
        "internalInstruction": "Present relevant products or services that align with the user’s stated needs and budget. Highlight the key features and benefits that solve their specific challenges.",
        "sampleUserMessage": "Based on what you’ve shared, here’s a solution I think would be a great fit. Let me walk you through its key features."
    },
    "Objection Handling": {
        "internalInstruction": "Address any concerns, hesitations, or objections the user may have (e.g., price, features, brand trust). Provide clarifications, comparisons, or alternatives as needed.",
        "sampleUserMessage": "I understand your concern about the price. Could you share more about what’s most important to you—cost, quality, or brand reputation?"
    },
    "Closing": {
        "internalInstruction": "Guide the user toward making a final decision. Summarize the benefits, clarify any final details, and ask for the sale or the next step (e.g., scheduling a demo or sending a proposal).",
        "sampleUserMessage": "We’ve covered a lot of details. Would you like to proceed with this option, or is there anything else you’d like to clarify before finalizing?"
    },
    "Follow-Up": {
        "internalInstruction": "If the sale isn’t immediately closed, schedule a follow-up or provide additional resources. Stay in touch to nurture the lead until they’re ready to buy.",
        "sampleUserMessage": "No worries if you’re not ready right now. Would you like me to send more information or schedule a follow-up chat for next week?"
    }
}

  return ", ".join(workflow_data.keys())

def get_workflow(current_stage):
    workflow_data = get_workflow_stages()
    if workflow_data:
        workflow_dict = json.loads(workflow_data)
        # Safely get the stage if it exists, otherwise return None
        return workflow_dict.get(current_stage, {}).get("internalInstruction", None)
    return None

from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)

# content = "Chào bạn"
# chat_history = []
# user_profile = ""

# import time

# start = time.time()
# rephrased_query = contextualize_query_and_detect_phase(content, chat_history, user_profile)
# print(rephrased_query)

# end = time.time()
# print(f"Time taken: {end - start:.4f} seconds")

from qdrant_client import QdrantClient
from qdrant_client.http.models import Filter, FieldCondition, MatchValue, SearchRequest
import time

def query_business_logic(
    qdrant_client: QdrantClient,
    embedding_model,
    query: str,
    workflow_stage: str = "Needs Assessment",
    collection_name: str = "business_logic",
    top_k: int = 5
):
    """
    Query the business_logic collection in Qdrant using semantic vector search
    and an optional filter by workflow_stage.

    Args:
        qdrant_client (QdrantClient): An instance of QdrantClient.
        embedding_model: An embedding model with .embed_query() method.
        query (str): The user query.
        workflow_stage (str): The stage to filter for (optional).
        collection_name (str): Name of the Qdrant collection.
        top_k (int): Number of results to return.

    Returns:
        list: A list of dictionaries containing strategy and score.
    """
    # Generate query embedding
    query_embedding = embedding_model.embed_query(query)

    # Build optional metadata filter
    query_filter = Filter(
        must=[
            FieldCondition(
                key="workflow_stage",
                match=MatchValue(value=workflow_stage)
            )
        ]
    )

    # Final query
    # Query Qdrant and track performance
    start = time.time()

    results = qdrant_client.query_points(
        collection_name="business_logic",
        query=query_embedding,
        limit=5,
        query_filter=query_filter  # Optional
    )

    end = time.time()

    print(f"\n🔍 Business Logic Results for: \"{query}\" [Stage: {workflow_stage}]")
    print(f"⏱️ Time taken: {end - start:.4f} seconds")

    output = []
    for point in results.points:
        strategy = point.payload.get("strategy", "No strategy")
        print(f"- {strategy} (Score: {point.score:.4f})")
        output.append({"strategy": strategy, "score": point.score})

    return output

# query = rephrased_query["rephrased_query"]
# workflow_stage = rephrased_query["workflow_stage"]

# logics = query_business_logic(qdrant_client, embedding_model, query, workflow_stage)

from typing import List

def rephrase_with_strategy(rephrased_query: str, chat_history: List[dict], strategies: List[str], llm) -> str:
    strategies_text = "\n".join([f"- {s['strategy']}" for s in strategies])

    prompt = f"""
      ### ROLE
      Bạn là một AI tư vấn bán hàng thông minh cho thiết bị âm thanh.

      ### CONTEXT
      Khách hàng đang trò chuyện với chatbot. Dựa trên nội dung cuộc hội thoại và các chiến lược bán hàng nội bộ (ví dụ: ưu tiên khuyến mãi, hỏi về nhu cầu...), bạn cần diễn đạt lại câu hỏi khách hàng sao cho:
      - Tự nhiên, rõ ràng
      - Phù hợp với chiến lược bán hàng
      - **Không trả lời câu hỏi**
      - Không giới thiệu sản phẩm

      ### TASK
      1. Diễn đạt lại câu hỏi khách hàng.
      2. Áp dụng các chiến lược bán hàng đã cung cấp.
      3. Giữ nguyên ý định nhưng thêm định hướng bán hàng.

      ### INPUTS
      Chiến lược bán hàng:
      {strategies_text}

      Lịch sử hội thoại gần nhất:
      {chat_history}

      Câu hỏi hiện tại của khách hàng:
      "{rephrased_query}"

      ### FEW-SHOT EXAMPLES

      #### Ví dụ 1:
      Câu gốc: "Loa nào tốt vậy?"
      Chiến lược:
      - Gợi ý sản phẩm đang khuyến mãi
      - Hỏi thêm về không gian sử dụng

      ⟶ Kết quả: "Tôi đang tìm loa chất lượng, có mẫu nào đang khuyến mãi không? Tôi định dùng trong phòng khách."

      #### Ví dụ 2:
      Câu gốc: "Bose 201 giá sao?"
      Chiến lược:
      - Tập trung sản phẩm bán chạy
      - Gợi ý khuyến mãi

      ⟶ Kết quả: "Tôi đang quan tâm đến Bose 201 – có khuyến mãi gì không?"

      #### Ví dụ xấu:
      Câu gốc: "Tôi đang tìm loa bluetooth"
      Chiến lược: hỏi thêm về mục đích sử dụng

      ⟶ Sai: "Tôi đang tìm loa bluetooth. Tôi sẽ sử dụng ở đâu?" ❌
      ⟶ Đúng: "Tôi muốn mua loa bluetooth dùng cho phòng khách, hiện có khuyến mãi gì không?" ✅


      ### FINAL OUTPUT
      Không được thêm những câu hỏi không tự nhiên hoặc khiến người dùng như đang nói chuyện với chính mình.
      Chỉ thêm thông tin định hướng khi hợp lý, và luôn giữ giọng điệu giống người dùng thật.
      """
    # print("strategy prompt:", prompt)

    response = llm.invoke(prompt)

    return response

# content = rephrase_with_strategy(query, chat_history, logics, llm).content
# print("content:", content)

from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)

content = "Tôi muốn mua loa"
chat_history = []
user_profile = ""

import time
# sum time

sum_time_start = time.time()

start = time.time()
rephrased_query = contextualize_query_and_detect_phase(content, chat_history, user_profile)
# print(rephrased_query)

end = time.time()
print(f"Time taken: {end - start:.4f} seconds")

query = rephrased_query["rephrased_query"]
workflow_stage = rephrased_query["workflow_stage"]

start = time.time()
logics = query_business_logic(qdrant_client, embedding_model, query, workflow_stage)
end = time.time()
print(f"Time taken: {end - start:.4f} seconds")

start = time.time()
content = rephrase_with_strategy(query, chat_history, logics, llm).content
print("rephrase_with_strategy:", content)
end = time.time()
print(f"Time taken: {end - start:.4f} seconds")

print(f"Sum Time taken: {end - sum_time_start:.4f} seconds")