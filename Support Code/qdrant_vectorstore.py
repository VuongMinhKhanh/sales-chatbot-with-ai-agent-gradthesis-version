# -*- coding: utf-8 -*-
"""Qdrant vectorstore.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_wdwSb0B4JOZIeSsA9486eVOR9dpBWKk
"""

!pip install qdrant-client

!pip install gspread oauth2client langchain_openai

!pip install fastembed

# --- Configuration ---
QDRANT_CLOUD_URL = ""
QDRANT_API_KEY = ""
SPREADSHEET_FILE = "User Feedback"  # your structured spreadsheet
COLLECTION_NAME = "user_feedback"
VECTOR_SIZE = 3072
BATCH_SIZE = 32

import os
os.environ["OPENAI_API_KEY"] = ""

from qdrant_client import QdrantClient, models

# --- 1. Connect to Qdrant ---
# If you're using Qdrant Cloud:
qdrant_client = QdrantClient(url=QDRANT_CLOUD_URL, api_key=QDRANT_API_KEY)

# --- 2. Create Collection ---
from qdrant_client.http.models import CollectionStatus, PointStruct

try:
    collection_info = qdrant_client.get_collection(collection_name=COLLECTION_NAME)
    if collection_info.status != CollectionStatus.GREEN:
        print(f"Collection {COLLECTION_NAME} exists but is not ready. Waiting...")
    else:
        print(f"Collection {COLLECTION_NAME} already exists")
except Exception:
    qdrant_client.create_collection(
        collection_name=COLLECTION_NAME,
        vectors_config=models.VectorParams(size=VECTOR_SIZE, distance=models.Distance.COSINE)
    )
    print(f"Collection {COLLECTION_NAME} is created")

# # --- 3. Load Spreadsheet ---
# from google.colab import drive
# drive.mount("/content/drive")

# import pandas as pd

# df = pd.read_excel(f"/content/drive/MyDrive/{SPREADSHEET_FILE}")
# # df.head()

# --- 3. Load Spreadsheet ---
import gspread
from google.colab import auth
from oauth2client.client import GoogleCredentials
from google.auth import default
import csv

# Authenticate and create a client
auth.authenticate_user()
creds, _ = default()
gc = gspread.authorize(creds)

# Open the Google Sheets file
spreadsheet = gc.open("User Feedback")

worksheet = spreadsheet.worksheet("Enriched Feedback")
data = worksheet.get_all_values()

import pandas as pd

feedback_df = pd.DataFrame(data[1:], columns=data[0])

# --- 4. Load Embeddings ---
from langchain_openai import OpenAIEmbeddings

embedding_model = OpenAIEmbeddings(model="text-embedding-3-large")

# --- 5. Iterate, Embed, and Upsert for User Feedback ---
points = []
for i in range(0, len(feedback_df), BATCH_SIZE):
    batch_df = feedback_df.iloc[i: i + BATCH_SIZE]

    # Construct text to embed by combining key fields from the user feedback record.
    texts = [
        f"Query: {row['Query']} Feedback: {row['Feedback']} Correction: {row['Correction']}"
        for _, row in batch_df.iterrows()
    ]

    # Compute embeddings for the batch of texts.
    embeddings = [embedding_model.embed_query(text) for text in texts]

    for j, embedding in enumerate(embeddings):
        row = batch_df.iloc[j]
        # Build a payload with relevant metadata
        payload = {
            # Use a unique id; if 'id' column doesn't exist, use the batch index as fallback.
            "id": row.get("id", i + j),
            "workflow_stage": row["workflow_stage"],
            "Query": row["Query"],
            "Response": row["Response"],
            "Feedback": row["Feedback"],
            "Correction": row["Correction"],
            "timestamp": row["timestamp"]
        }

        points.append(
            PointStruct(
                id=i + j,
                vector=embedding,
                payload=payload
            )
        )

    print(f"Processed batch {i // BATCH_SIZE + 1}")

# === 6. Upsert to Qdrant ===
qdrant_client.upsert(collection_name=COLLECTION_NAME, wait=True, points=points)
print(f"‚úÖ Uploaded {len(points)} user feedback to Qdrant.")

# === 7. Example Query (Optional) ===
query = "cho t√¥i xem h√†ng v·ªõi"
query_embedding = embedding_model.embed_query(query)

from qdrant_client.http.models import Filter, FieldCondition, MatchValue

# Optional filter by workflow_stage
query_filter = Filter(
    must=[
        FieldCondition(
            key="workflow_stage",
            match=MatchValue(value="Qualification")
        )
    ]
)

# Final query
results = qdrant_client.query_points(
    collection_name=COLLECTION_NAME,
    query=query_embedding,
    limit=5,
    query_filter=query_filter  # Optional
)

import time

start = time.time()

print("Results:")
for result in results.points:
    print(f"- {result.payload} (Score: {result.score:.4f})")

end = time.time()
print(f"Time taken: {end - start:.4f} seconds")

"""#Test with AI Agents"""

!pip install -q langchain langchain_community gspread oauth2client langchain_experimental chromadb langchain_openai faiss-cpu

!pip install -q weaviate-client langchain-weaviate

premise = """
You are a highly qualified sales consultant specializing in electronic music equipment. You are working for 769Audio, one of the top three distributors in Ho Chi Minh City. Your goal is to provide accurate, helpful, and professional responses to customers based on the provided database.

---

### **Role:**
- Your primary role is to assist customers in selecting and understanding products from the database.

---

### **Context:**
- You are communicating with customers in Vietnamese unless they explicitly request another language.
- Always base your answers on the provided database. Do not generate any additional or speculative information.

---

### **Key Instructions:**

#### **General Product Queries:**
- Use the product name in the `"T√™n"` column to identify matches, even if phrased differently by the customer.
- Whenever **a product is mentioned**, always provide **two types of links** for products:
  1. The main product link ("Link s·∫£n ph·∫©m").
  2. Additional image links from `"T·∫≠p link ·∫£nh"`. Include up to 3 relevant links if available.
- Format image links neatly in Markdown. Example:
B·∫°n c√≥ th·ªÉ xem chi ti·∫øt v√† h√¨nh ·∫£nh s·∫£n ph·∫©m t·∫°i ƒë√¢y:
**Link s·∫£n ph·∫©m:**
[Loa JBL Pasion 10](https://saigonaudio.com.vn/san-pham/123/loa-jbl-pasion-10.html)

**H√¨nh ·∫£nh s·∫£n ph·∫©m:**
[![H√¨nh 1](https://saigonaudio.com.vn/upload/images/Loa-JBL-Pasion-10-01.jpg)](https://saigonaudio.com.vn/upload/images/Loa-JBL-Pasion-10-01.jpg)
[![H√¨nh 2](https://saigonaudio.com.vn/upload/images/Loa-JBL-Pasion-10-02.jpg)](https://saigonaudio.com.vn/upload/images/Loa-JBL-Pasion-10-02.jpg)

#### **Price Details:**
- When quoting prices, prioritize promotions:
1. Check the `"Khuy·∫øn m√£i"` column. If the value is `1`, the product is on discount.
2. Look in the `"N·ªôi dung"` column for any additional promotion details.
3. Quote both the original price from the `"Gi√° g·ªëc"` column and the discounted price if applicable.

#### **Availability:**
- Only recommend products where the `"Hi·ªÉn th·ªã"` column indicates `1` (visible) or the `"T√¨nh tr·∫°ng"` column indicates `1` (in stock).
- Politely inform the customer if a product is unavailable.

#### **Features and Specifications:**
- For questions about specifications like capacity or other attributes, first check the `"M√¥ t·∫£"` and `"N·ªôi dung"` columns.
- If no relevant details are found in these columns, clearly state that the information is not available in the database.

#### **Country of Origin:**
- If a product is manufactured in China, first describe it as imported. If the customer asks for more detail, directly mention "manufactured in China."

---

### **Markdown and Formatting:**
- Format your responses neatly using line breaks and Markdown where appropriate.
- When providing image links, render the image in Markdown so it is visible in the chat, and make the image clickable by embedding it with a link.
- Example:
Question: "What is the price of Loa JBL 201 Series 4?"
Response: D·∫°, hi·ªán t·∫°i, gi√° c·ªßa loa l√† 3,500,000‚Ç´. Hi·ªán ƒëang gi·∫£m c√≤n 3,000,000‚Ç´:
**Link s·∫£n ph·∫©m:**
[Loa JBL 201 Series 4](https://saigonaudio.com.vn/san-pham/201/loa-jbl-201-series-4.html)

**H√¨nh ·∫£nh s·∫£n ph·∫©m:**
[![H√¨nh 1](https://saigonaudio.com.vn/upload/images/Loa-JBL-201-Series-4-01.jpg)](https://saigonaudio.com.vn/upload/images/Loa-JBL-201-Series-4-01.jpg)
[![H√¨nh 2](https://saigonaudio.com.vn/upload/images/Loa-JBL-201-Series-4-02.jpg)](https://saigonaudio.com.vn/upload/images/Loa-JBL-201-Series-4-02.jpg)

---

### **Customer-Centric Assistance:**
- Always prioritize promotions and discounts when suggesting products.
- If a customer indicates they want to purchase something, suggest relevant items first based on their query.
- If the customer specifies a price range, look for products with prices closest to their range.
- Use examples from the database for clarity. Always provide relevant URLs for products.

---

### **Advanced Document Understanding:**
- When searching for discount or promotion details:
  - Look at the metadata `source: "Khuy·∫øn m√£i"`. If the value is `1`, the product has a discount.
  - Cross-reference the metadata `source: "N·ªôi dung"` for additional details about promotions or discounts.

- If the question involves finding related specifications:
  - First prioritize `"M√¥ t·∫£"` and `"N·ªôi dung"`.
  - If these do not contain the required details, inform the customer.

- If a product image is requested:
  - Provide the link from `"T·∫≠p link ·∫£nh"` or `"·∫¢nh ch√≠nh"` or `"·∫¢nh nh·ªè"`.
  - Example: "Here's the image: [![Image](https://saigonaudio.com.vn/upload/images/<·∫¢nh ch√≠nh>.jpg)](https://saigonaudio.com.vn/upload/images/<·∫¢nh ch√≠nh>.jpg)."

---

### **Notes:**
- Always prioritize database information. Never guess or fabricate data.
- Clearly admit when no relevant information is available: "Xin l·ªói, nh∆∞ng hi·ªán t·∫°i t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p..."
- Respond succinctly, professionally, and engagingly.
"""

import json
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

def contextualize_query_and_detect_phase(query, chat_history, user_profile):
    """
    Rephrases a user's query for clarity and determines the sales workflow stage.

    The prompt instructs the LLM to:
      - Rephrase the customer's query using the conversation history.
      - Inject discount keywords when customers ask for product suggestions.
      - Analyze the chat history and user profile to decide the current sales stage.
      - Always respond in Vietnamese (unless the customer specifies otherwise).

    Few-shot examples (in Vietnamese) are provided to illustrate the expected behavior.
    Expected JSON response format:
      {"rephrased_query": "‚Ä¶", "workflow_stage": "‚Ä¶"}
    """

    # Combined unified prompt with instructions, examples, and a JSON sample.
    unified_system_prompt = f"""
B·∫°n l√† m·ªôt chuy√™n gia t∆∞ v·∫•n b√°n h√†ng thi·∫øt b·ªã √¢m thanh (loa, micro, mixer, ampli,...) v·ªõi ki·∫øn th·ª©c s√¢u v·ªÅ quy tr√¨nh b√°n h√†ng. Quy tr√¨nh c·ªßa ch√∫ng ta g·ªìm c√°c giai ƒëo·∫°n: {get_workflow_stages()}.
    """ + """
H√£y th·ª±c hi·ªán c√°c nhi·ªám v·ª• sau:
D·ª±a tr√™n l·ªãch s·ª≠ tr√≤ chuy·ªán d∆∞·ªõi ƒë√¢y v√† c√¢u h·ªèi m·ªõi nh·∫•t c·ªßa kh√°ch h√†ng, h√£y di·ªÖn gi·∫£i c√¢u h·ªèi sao cho d·ªÖ hi·ªÉu v√† li√™n quan ƒë·∫øn ng·ªØ c·∫£nh ƒë√£ trao ƒë·ªïi.

1. S·ª≠ d·ª•ng th√¥ng tin t·ª´ l·ªãch s·ª≠ ƒë·ªÉ th√™m chi ti·∫øt c√≤n thi·∫øu cho c√¢u h·ªèi m·ªõi (n·∫øu c√≥).
2. ƒê·∫£m b·∫£o r·∫±ng c√¢u h·ªèi ƒë∆∞·ª£c di·ªÖn gi·∫£i m·ªôt c√°ch ch√≠nh x√°c v√† ng·∫Øn g·ªçn, nh∆∞ng v·∫´n gi·ªØ ng·ªØ c·∫£nh t·ª´ l·ªãch s·ª≠ tr√≤ chuy·ªán.
3. N·∫øu kh√¥ng c√≥ ƒë·ªß th√¥ng tin t·ª´ l·ªãch s·ª≠, h√£y di·ªÖn gi·∫£i c√¢u h·ªèi m·ªõi sao cho d·ªÖ hi·ªÉu nh·∫•t m√† kh√¥ng c·∫ßn ng·ªØ c·∫£nh.
4. Ph√¢n t√≠ch l·ªãch s·ª≠ tr√≤ chuy·ªán v√† h·ªì s∆° kh√°ch h√†ng ƒë·ªÉ x√°c ƒë·ªãnh giai ƒëo·∫°n hi·ªán t·∫°i c·ªßa quy tr√¨nh b√°n h√†ng.
5. N·∫øu l·ªãch s·ª≠ cho th·∫•y ƒë√£ c√≥ 2‚Äì3 v√≤ng trao ƒë·ªïi (v√≠ d·ª•: nhi·ªÅu t∆∞∆°ng t√°c ·ªü giai ƒëo·∫°n Needs Assessment ho·∫∑c Qualification), h√£y d·ª± ƒëo√°n cu·ªôc tr√≤ chuy·ªán chuy·ªÉn sang giai ƒëo·∫°n Product Presentation.
6. N·∫øu kh√¥ng ƒë·ªß th√¥ng tin t·ª´ l·ªãch s·ª≠, h√£y t√°i di·ªÖn ƒë·∫°t l·∫°i c√¢u h·ªèi m·ªõi sao cho ƒë∆°n gi·∫£n v√† d·ªÖ hi·ªÉu.

V√≠ d·ª•:
- V√≠ d·ª• 1:
  + L·ªãch s·ª≠: "Cho t√¥i bi·∫øt gi√° loa JBL 201 Series 4"
  + C√¢u h·ªèi m·ªõi: "G·ª≠i link s·∫£n ph·∫©m"
  + Di·ªÖn gi·∫£i (k·∫øt qu·∫£ mong ƒë·ª£i): {{"rephrased_query": "G·ª≠i link s·∫£n ph·∫©m", "workflow_stage": "Needs Assessment"}}

- V√≠ d·ª• 2:
  + L·ªãch s·ª≠: "T√¥i mu·ªën mua micro cao c·∫•p"
  + C√¢u h·ªèi m·ªõi: "Bao nhi√™u ti·ªÅn micro?"
  + Di·ªÖn gi·∫£i (k·∫øt qu·∫£ mong ƒë·ª£i): {{"rephrased_query": "Bao nhi√™u ti·ªÅn micro?", "workflow_stage": "Needs Assessment"}}

L·ªãch s·ª≠ tr√≤ chuy·ªán:
{chat_history}

M√¥ t·∫£ kh√°ch h√†ng:
{user_profile}

C√¢u h·ªèi m·ªõi:
{input}

Tr·∫£ l·ªùi c·ªßa b·∫°n ph·∫£i l√† m·ªôt ƒë·ªëi t∆∞·ª£ng JSON v·ªõi ƒë·ªãnh d·∫°ng:
{{"rephrased_query": "‚Ä¶", "workflow_stage": "‚Ä¶"}}
    """

    # Build the unified prompt with a single system message and placeholders.
    unified_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", unified_system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),  # Placeholder for dynamic chat history
            ("system", "{user_profile}"),  # Inject the user profile into the prompt
            ("human", "{input}"),
        ]
    )

    def contextualize_query(query, history, user_profile):
        prompt = unified_prompt.format(input=query, chat_history=history, user_profile=user_profile)
        return llm.invoke(prompt)  # 'llm' is your language model interface

    # Invoke the language model and parse the JSON response.
    contextualized_query = contextualize_query(query, chat_history, user_profile)
    json_data = json.loads(contextualized_query.content)
    return json_data


def get_workflow_stages():
  workflow_data = {
    "Greeting": {
        "internalInstruction": "Greet the user warmly, introduce the service or product line, and invite them to share their needs or questions.",
        "sampleUserMessage": "Hello there! How can I assist you with your audio equipment needs today?"
    },
    "Needs Assessment": {
        "internalInstruction": "Ask targeted questions to uncover the user's main challenges, goals, or requirements. Focus on clarifying their exact needs.",
        "sampleUserMessage": "Could you tell me more about what you're looking for and any challenges you‚Äôre currently facing?"
    },
    "Qualification": {
        "internalInstruction": "Determine if the customer meets key criteria, such as budget, timeline, or decision-making authority. Gather enough info to decide if they‚Äôre a good fit.",
        "sampleUserMessage": "Do you have a budget range or timeframe in mind? And are you the one making the final decision?"
    },
    "Presentation": {
        "internalInstruction": "Present relevant products or services that align with the user‚Äôs stated needs and budget. Highlight the key features and benefits that solve their specific challenges.",
        "sampleUserMessage": "Based on what you‚Äôve shared, here‚Äôs a solution I think would be a great fit. Let me walk you through its key features."
    },
    "Objection Handling": {
        "internalInstruction": "Address any concerns, hesitations, or objections the user may have (e.g., price, features, brand trust). Provide clarifications, comparisons, or alternatives as needed.",
        "sampleUserMessage": "I understand your concern about the price. Could you share more about what‚Äôs most important to you‚Äîcost, quality, or brand reputation?"
    },
    "Closing": {
        "internalInstruction": "Guide the user toward making a final decision. Summarize the benefits, clarify any final details, and ask for the sale or the next step (e.g., scheduling a demo or sending a proposal).",
        "sampleUserMessage": "We‚Äôve covered a lot of details. Would you like to proceed with this option, or is there anything else you‚Äôd like to clarify before finalizing?"
    },
    "Follow-Up": {
        "internalInstruction": "If the sale isn‚Äôt immediately closed, schedule a follow-up or provide additional resources. Stay in touch to nurture the lead until they‚Äôre ready to buy.",
        "sampleUserMessage": "No worries if you‚Äôre not ready right now. Would you like me to send more information or schedule a follow-up chat for next week?"
    }
}

  return ", ".join(workflow_data.keys())

def get_workflow(current_stage):
    workflow_data = get_workflow_stages()
    if workflow_data:
        workflow_dict = json.loads(workflow_data)
        # Safely get the stage if it exists, otherwise return None
        return workflow_dict.get(current_stage, {}).get("internalInstruction", None)
    return None

from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)

# content = "Ch√†o b·∫°n"
# chat_history = []
# user_profile = ""

# import time

# start = time.time()
# rephrased_query = contextualize_query_and_detect_phase(content, chat_history, user_profile)
# print(rephrased_query)

# end = time.time()
# print(f"Time taken: {end - start:.4f} seconds")

from qdrant_client import QdrantClient
from qdrant_client.http.models import Filter, FieldCondition, MatchValue, SearchRequest
import time

def query_business_logic(
    qdrant_client: QdrantClient,
    embedding_model,
    query: str,
    workflow_stage: str = "Needs Assessment",
    collection_name: str = "business_logic",
    top_k: int = 5
):
    """
    Query the business_logic collection in Qdrant using semantic vector search
    and an optional filter by workflow_stage.

    Args:
        qdrant_client (QdrantClient): An instance of QdrantClient.
        embedding_model: An embedding model with .embed_query() method.
        query (str): The user query.
        workflow_stage (str): The stage to filter for (optional).
        collection_name (str): Name of the Qdrant collection.
        top_k (int): Number of results to return.

    Returns:
        list: A list of dictionaries containing strategy and score.
    """
    # Generate query embedding
    query_embedding = embedding_model.embed_query(query)

    # Build optional metadata filter
    query_filter = Filter(
        must=[
            FieldCondition(
                key="workflow_stage",
                match=MatchValue(value=workflow_stage)
            )
        ]
    )

    # Final query
    # Query Qdrant and track performance
    start = time.time()

    results = qdrant_client.query_points(
        collection_name="business_logic",
        query=query_embedding,
        limit=5,
        query_filter=query_filter  # Optional
    )

    end = time.time()

    print(f"\nüîç Business Logic Results for: \"{query}\" [Stage: {workflow_stage}]")
    print(f"‚è±Ô∏è Time taken: {end - start:.4f} seconds")

    output = []
    for point in results.points:
        strategy = point.payload.get("strategy", "No strategy")
        print(f"- {strategy} (Score: {point.score:.4f})")
        output.append({"strategy": strategy, "score": point.score})

    return output

# query = rephrased_query["rephrased_query"]
# workflow_stage = rephrased_query["workflow_stage"]

# logics = query_business_logic(qdrant_client, embedding_model, query, workflow_stage)

from typing import List

def rephrase_with_strategy(rephrased_query: str, chat_history: List[dict], strategies: List[str], llm) -> str:
    strategies_text = "\n".join([f"- {s['strategy']}" for s in strategies])

    prompt = f"""
      ### ROLE
      B·∫°n l√† m·ªôt AI t∆∞ v·∫•n b√°n h√†ng th√¥ng minh cho thi·∫øt b·ªã √¢m thanh.

      ### CONTEXT
      Kh√°ch h√†ng ƒëang tr√≤ chuy·ªán v·ªõi chatbot. D·ª±a tr√™n n·ªôi dung cu·ªôc h·ªôi tho·∫°i v√† c√°c chi·∫øn l∆∞·ª£c b√°n h√†ng n·ªôi b·ªô (v√≠ d·ª•: ∆∞u ti√™n khuy·∫øn m√£i, h·ªèi v·ªÅ nhu c·∫ßu...), b·∫°n c·∫ßn di·ªÖn ƒë·∫°t l·∫°i c√¢u h·ªèi kh√°ch h√†ng sao cho:
      - T·ª± nhi√™n, r√µ r√†ng
      - Ph√π h·ª£p v·ªõi chi·∫øn l∆∞·ª£c b√°n h√†ng
      - **Kh√¥ng tr·∫£ l·ªùi c√¢u h·ªèi**
      - Kh√¥ng gi·ªõi thi·ªáu s·∫£n ph·∫©m

      ### TASK
      1. Di·ªÖn ƒë·∫°t l·∫°i c√¢u h·ªèi kh√°ch h√†ng.
      2. √Åp d·ª•ng c√°c chi·∫øn l∆∞·ª£c b√°n h√†ng ƒë√£ cung c·∫•p.
      3. Gi·ªØ nguy√™n √Ω ƒë·ªãnh nh∆∞ng th√™m ƒë·ªãnh h∆∞·ªõng b√°n h√†ng.

      ### INPUTS
      Chi·∫øn l∆∞·ª£c b√°n h√†ng:
      {strategies_text}

      L·ªãch s·ª≠ h·ªôi tho·∫°i g·∫ßn nh·∫•t:
      {chat_history}

      C√¢u h·ªèi hi·ªán t·∫°i c·ªßa kh√°ch h√†ng:
      "{rephrased_query}"

      ### FEW-SHOT EXAMPLES

      #### V√≠ d·ª• 1:
      C√¢u g·ªëc: "Loa n√†o t·ªët v·∫≠y?"
      Chi·∫øn l∆∞·ª£c:
      - G·ª£i √Ω s·∫£n ph·∫©m ƒëang khuy·∫øn m√£i
      - H·ªèi th√™m v·ªÅ kh√¥ng gian s·ª≠ d·ª•ng

      ‚ü∂ K·∫øt qu·∫£: "T√¥i ƒëang t√¨m loa ch·∫•t l∆∞·ª£ng, c√≥ m·∫´u n√†o ƒëang khuy·∫øn m√£i kh√¥ng? T√¥i ƒë·ªãnh d√πng trong ph√≤ng kh√°ch."

      #### V√≠ d·ª• 2:
      C√¢u g·ªëc: "Bose 201 gi√° sao?"
      Chi·∫øn l∆∞·ª£c:
      - T·∫≠p trung s·∫£n ph·∫©m b√°n ch·∫°y
      - G·ª£i √Ω khuy·∫øn m√£i

      ‚ü∂ K·∫øt qu·∫£: "T√¥i ƒëang quan t√¢m ƒë·∫øn Bose 201 ‚Äì c√≥ khuy·∫øn m√£i g√¨ kh√¥ng?"

      #### V√≠ d·ª• x·∫•u:
      C√¢u g·ªëc: "T√¥i ƒëang t√¨m loa bluetooth"
      Chi·∫øn l∆∞·ª£c: h·ªèi th√™m v·ªÅ m·ª•c ƒë√≠ch s·ª≠ d·ª•ng

      ‚ü∂ Sai: "T√¥i ƒëang t√¨m loa bluetooth. T√¥i s·∫Ω s·ª≠ d·ª•ng ·ªü ƒë√¢u?" ‚ùå
      ‚ü∂ ƒê√∫ng: "T√¥i mu·ªën mua loa bluetooth d√πng cho ph√≤ng kh√°ch, hi·ªán c√≥ khuy·∫øn m√£i g√¨ kh√¥ng?" ‚úÖ


      ### FINAL OUTPUT
      Kh√¥ng ƒë∆∞·ª£c th√™m nh·ªØng c√¢u h·ªèi kh√¥ng t·ª± nhi√™n ho·∫∑c khi·∫øn ng∆∞·ªùi d√πng nh∆∞ ƒëang n√≥i chuy·ªán v·ªõi ch√≠nh m√¨nh.
      Ch·ªâ th√™m th√¥ng tin ƒë·ªãnh h∆∞·ªõng khi h·ª£p l√Ω, v√† lu√¥n gi·ªØ gi·ªçng ƒëi·ªáu gi·ªëng ng∆∞·ªùi d√πng th·∫≠t.
      """
    # print("strategy prompt:", prompt)

    response = llm.invoke(prompt)

    return response

# content = rephrase_with_strategy(query, chat_history, logics, llm).content
# print("content:", content)

from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)

content = "T√¥i mu·ªën mua loa"
chat_history = []
user_profile = ""

import time
# sum time

sum_time_start = time.time()

start = time.time()
rephrased_query = contextualize_query_and_detect_phase(content, chat_history, user_profile)
# print(rephrased_query)

end = time.time()
print(f"Time taken: {end - start:.4f} seconds")

query = rephrased_query["rephrased_query"]
workflow_stage = rephrased_query["workflow_stage"]

start = time.time()
logics = query_business_logic(qdrant_client, embedding_model, query, workflow_stage)
end = time.time()
print(f"Time taken: {end - start:.4f} seconds")

start = time.time()
content = rephrase_with_strategy(query, chat_history, logics, llm).content
print("rephrase_with_strategy:", content)
end = time.time()
print(f"Time taken: {end - start:.4f} seconds")

print(f"Sum Time taken: {end - sum_time_start:.4f} seconds")